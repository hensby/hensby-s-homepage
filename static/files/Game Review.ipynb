{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data mining Term Project\n",
    "## Hengchao Wang 1001778272\n",
    "\n",
    "### Reference. \n",
    "BoardGameGeek Reviews Baseline Model\n",
    "https://www.kaggle.com/ellpeeaxe/boardgamegeek-reviews-baseline-model\n",
    "\n",
    "Word2vec In Supervised NLP Tasks. Shortcut\n",
    "https://www.kaggle.com/vladislavkisin/word2vec-in-supervised-nlp-tasks-shortcut/comments\n",
    "\n",
    "Cuz the scale of the dataset is super big. Cannot use one hot expression to exprese words and sentences. It will cause the curse of dimensionality. Which means the matrix is big and sparse to be compute. So I decide to use Word2Vec word embedding model to reduce dimension of matrix. I have two references. The link is shown above. \n",
    "\n",
    "The based task of this question is a regression problem. The imput data is 300-dimensional word vector, output is the prediction of rate for each review."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "import nltk\n",
    "import re,string,unicodedata\n",
    "import seaborn as sns\n",
    "import gensim\n",
    "import sklearn\n",
    "\n",
    "from pandas import Series\n",
    "from wordcloud import WordCloud,STOPWORDS\n",
    "from bs4 import BeautifulSoup\n",
    "from nltk.tokenize.toktok import ToktokTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from gensim.models import word2vec, Word2Vec\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.linear_model import LinearRegression, BayesianRidge\n",
    "import joblib\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from sklearn import ensemble"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get data from csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rating</th>\n",
       "      <th>comment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10.0</td>\n",
       "      <td>Currently, this sits on my list as my favorite...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10.0</td>\n",
       "      <td>I know it says how many plays, but many, many ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   rating                                            comment\n",
       "0    10.0                                                NaN\n",
       "1    10.0                                                NaN\n",
       "2    10.0  Currently, this sits on my list as my favorite...\n",
       "3    10.0  I know it says how many plays, but many, many ...\n",
       "4    10.0                                                NaN"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get review and rating columns\n",
    "review_path = 'bgg-13m-reviews.csv'\n",
    "\n",
    "data = pd.read_csv(review_path, usecols=[2,3])\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rate</th>\n",
       "      <th>comment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10.0</td>\n",
       "      <td>currently , thi sit list favorit game .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10.0</td>\n",
       "      <td>know say mani plays , many , mani uncounted. l...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10.0</td>\n",
       "      <td>never tire thi game .. awesom</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10.0</td>\n",
       "      <td>thi probabl best game ever played. requir thin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10.0</td>\n",
       "      <td>fantast game. got hook game .</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   rate                                            comment\n",
       "0  10.0            currently , thi sit list favorit game .\n",
       "1  10.0  know say mani plays , many , mani uncounted. l...\n",
       "2  10.0                      never tire thi game .. awesom\n",
       "3  10.0  thi probabl best game ever played. requir thin...\n",
       "4  10.0                      fantast game. got hook game ."
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# remove null comment\n",
    "def remove_nan(data):\n",
    "    data['comment']=data['comment'].fillna('null')\n",
    "    data = data[~data['comment'].isin(['null'])]\n",
    "    data = data.reset_index(drop=True)\n",
    "    return data\n",
    "data = remove_nan(data)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**This is data describtion. The number of review is 2.637756e+06**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>2.637756e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>6.852070e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>1.775769e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.401300e-45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>6.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>7.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>8.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.000000e+01</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             rating\n",
       "count  2.637756e+06\n",
       "mean   6.852070e+00\n",
       "std    1.775769e+00\n",
       "min    1.401300e-45\n",
       "25%    6.000000e+00\n",
       "50%    7.000000e+00\n",
       "75%    8.000000e+00\n",
       "max    1.000000e+01"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preprocessing\n",
    "\n",
    "For data preprocessing I using **tokenizer()** from **NLTK** library to tokenize the words. Load stopword from **NLTK** and load html strips from **beautifulsoup4** library. Use regular expression to remove them and some special characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tokenization of text\n",
    "tokenizer=ToktokTokenizer()\n",
    "#Setting English stopwords\n",
    "stopword_list=nltk.corpus.stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Removing the html strips\n",
    "def strip_html(text):\n",
    "    soup = BeautifulSoup(text, \"html.parser\")\n",
    "    return soup.get_text()\n",
    "\n",
    "#Removing the square brackets\n",
    "def remove_between_square_brackets(text):\n",
    "    return re.sub('\\[[^]]*\\]', '', text)\n",
    "\n",
    "#Removing the noisy text\n",
    "def denoise_text(text):\n",
    "    text = strip_html(text)\n",
    "    text = remove_between_square_brackets(text)\n",
    "    return text\n",
    "#Apply function on review column\n",
    "data['comment']=data['comment'].apply(remove_between_square_brackets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define function for removing special characters\n",
    "def remove_special_characters(text, remove_digits=True):\n",
    "    pattern=r'[^a-zA-z0-9\\s]'\n",
    "    text=re.sub(pattern,'',text)\n",
    "    return text\n",
    "#Apply function on review column\n",
    "data['comment']=data['comment'].apply(remove_special_characters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Stemming the text\n",
    "def simple_stemmer(text):\n",
    "    ps=nltk.porter.PorterStemmer()\n",
    "    text= ' '.join([ps.stem(word) for word in text.split()])\n",
    "    return text\n",
    "#Apply function on review column\n",
    "data['comment']=data['comment'].apply(simple_stemmer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"you'd\", \"that'll\", 'other', 'any', \"won't\", \"you're\", 'have', 'yourselves', 'about', 'm', 'were', 'our', 'than', 'their', 'haven', 'being', 'over', 't', 'been', 'against', 'again', 'we', 'most', 'doesn', 'so', 'yourself', \"aren't\", 'mustn', 'under', 'just', 'down', 'ma', 'with', 'until', 'isn', 'don', 'shan', \"shouldn't\", 'myself', \"you've\", 'having', 'has', 'between', 'because', 'was', 'yours', 'nor', 'am', 'through', 'his', 'as', 'few', 'but', 'and', 'before', 'itself', 'hers', 'during', \"mustn't\", 'y', 'doing', 'an', \"you'll\", 'they', 'hasn', 'did', 'each', \"couldn't\", 'ours', 'weren', 'hadn', 'there', 'then', \"doesn't\", 'that', 'this', 'needn', 'no', 'i', 'aren', 'too', 'once', 'you', 'themselves', 'her', 'these', 'll', 'won', 'out', 'how', \"it's\", 'herself', 'to', 'when', 'o', 'my', 'of', 'into', \"didn't\", \"hadn't\", 'very', 'him', 'what', 'now', 'who', 'are', 'if', 'in', 'above', 'why', 'all', 'off', 'where', 'd', 'didn', 'couldn', 'while', 'does', 'she', 'wasn', 'theirs', 'the', 'your', \"should've\", 'by', 'up', 'whom', 'a', \"weren't\", 'same', \"hasn't\", 'mightn', \"shan't\", 'some', 'from', 'below', 're', 'which', 'those', \"don't\", \"mightn't\", 'will', 'its', 'only', \"needn't\", 'himself', 's', 'more', 'such', 'not', 'he', 'on', 'own', \"she's\", 'is', \"haven't\", 'be', 've', 'further', 'do', 'should', 'them', 'had', \"wasn't\", 'me', 'both', 'shouldn', 'or', 'can', 'for', 'ain', 'it', 'ourselves', \"isn't\", \"wouldn't\", 'here', 'at', 'after', 'wouldn'}\n"
     ]
    }
   ],
   "source": [
    "#set stopwords to english\n",
    "stop=set(stopwords.words('english'))\n",
    "print(stop)\n",
    "\n",
    "#removing the stopwords\n",
    "def remove_stopwords(text, is_lower_case=False):\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    tokens = [token.strip() for token in tokens]\n",
    "    if is_lower_case:\n",
    "        filtered_tokens = [token for token in tokens if token not in stopword_list]\n",
    "    else:\n",
    "        filtered_tokens = [token for token in tokens if token.lower() not in stopword_list]\n",
    "    filtered_text = ' '.join(filtered_tokens)    \n",
    "    return filtered_text\n",
    "#Apply function on review column\n",
    "data['comment']=data['comment'].apply(remove_stopwords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**After we remove the stopword we need to remove the empty review again because come short review after remove stopword will change into empty.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = remove_nan(data)\n",
    "data.to_csv('data_after_remove_st.csv', header=False, index=False, encoding = 'utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = ['rate', 'comment']\n",
    "data = pd.read_csv('data_after_remove_st.csv',names = columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['comment'] = data.comment.str.lower()\n",
    "data['document_sentences'] = data.comment.str.split('.') \n",
    "# data['tokenized_sentences'] = data['document_sentences']\n",
    "data['tokenized_sentences'] = list(map(lambda sentences:list(map(nltk.word_tokenize, sentences)),data.document_sentences))  \n",
    "data['tokenized_sentences'] = list(map(lambda sentences: list(filter(lambda lst: lst, sentences)), data.tokenized_sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rate</th>\n",
       "      <th>comment</th>\n",
       "      <th>document_sentences</th>\n",
       "      <th>tokenized_sentences</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>993001</th>\n",
       "      <td>7.0</td>\n",
       "      <td>good deduct game , go wrong question answer wr...</td>\n",
       "      <td>[good deduct game , go wrong question answer w...</td>\n",
       "      <td>[[good, deduct, game, ,, go, wrong, question, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1965460</th>\n",
       "      <td>6.0</td>\n",
       "      <td>thi reason simpl area control game nice mechan...</td>\n",
       "      <td>[thi reason simpl area control game nice mecha...</td>\n",
       "      <td>[[thi, reason, simpl, area, control, game, nic...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>273330</th>\n",
       "      <td>7.8</td>\n",
       "      <td>awesom game. sleeved .</td>\n",
       "      <td>[awesom game,  sleeved , ]</td>\n",
       "      <td>[[awesom, game], [sleeved]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>579587</th>\n",
       "      <td>7.2</td>\n",
       "      <td>thi everyth want bang ! , except much streamli...</td>\n",
       "      <td>[thi everyth want bang ! , except much streaml...</td>\n",
       "      <td>[[thi, everyth, want, bang, !, ,, except, much...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>740450</th>\n",
       "      <td>6.0</td>\n",
       "      <td>fun parti trivia game .</td>\n",
       "      <td>[fun parti trivia game , ]</td>\n",
       "      <td>[[fun, parti, trivia, game]]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         rate                                            comment  \\\n",
       "993001    7.0  good deduct game , go wrong question answer wr...   \n",
       "1965460   6.0  thi reason simpl area control game nice mechan...   \n",
       "273330    7.8                             awesom game. sleeved .   \n",
       "579587    7.2  thi everyth want bang ! , except much streamli...   \n",
       "740450    6.0                            fun parti trivia game .   \n",
       "\n",
       "                                        document_sentences  \\\n",
       "993001   [good deduct game , go wrong question answer w...   \n",
       "1965460  [thi reason simpl area control game nice mecha...   \n",
       "273330                          [awesom game,  sleeved , ]   \n",
       "579587   [thi everyth want bang ! , except much streaml...   \n",
       "740450                          [fun parti trivia game , ]   \n",
       "\n",
       "                                       tokenized_sentences  \n",
       "993001   [[good, deduct, game, ,, go, wrong, question, ...  \n",
       "1965460  [[thi, reason, simpl, area, control, game, nic...  \n",
       "273330                         [[awesom, game], [sleeved]]  \n",
       "579587   [[thi, everyth, want, bang, !, ,, except, much...  \n",
       "740450                        [[fun, parti, trivia, game]]  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Challenge 1\n",
    "**Here is a hint: Because the String[] cannot save as csv. The tokenized_sentences after save into csv will change the format into String and cannot load again.** This is one of a challenge I met. At the first few round of training Word2Vec model. The final accuracy is super low. I check the word expression of each word. The output from Word2Vec is less than 0.0001. That means that these word almost doesn't appear in the dataset. That doesn't make sence. So I check the model. The model.wv.vocab.keys() is small too and the vocabelory are latters, not words. So it must be the split problem or the format problem. So I check the type of each variable. The type of \"tokenized_sentences\" is changes. After google the issue. I found the point is you cannot save string[] in csv.\n",
    "\n",
    "I wrote the wrong code as a comment in next 2 cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data.to_csv(\"data_after_pre.csv\",sep=',',index=False, encoding = 'utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data = pd.read_csv('data_after_pre.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next cell will not be run when I train the Word2Vec. I train the Word2Vec model by using the whole Dataset.\n",
    "The next cell will be run when I train the regression model. Cuz the computation I have only can use 50k reviews to train the regression model. So I use 10k and 50k reviews and compare them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take the top 10k after random ordering\n",
    "data = data.reindex(np.random.permutation(data.index))[:100000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the data into training data and test data.\n",
    "train, test, y_train, y_test = train_test_split(data, data['rate'], test_size=.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(train.tokenized_sentences[993001])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of sentences: 237600.\n",
      "Number of rows: 80000.\n"
     ]
    }
   ],
   "source": [
    "#Collecting a vocabulary\n",
    "voc = []\n",
    "for sentence in train.tokenized_sentences:\n",
    "    voc.extend(sentence)\n",
    "#     print(sentence)\n",
    "\n",
    "print(\"Number of sentences: {}.\".format(len(voc)))\n",
    "print(\"Number of rows: {}.\".format(len(train)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['(', 'vanilla', 'game', 'only'],\n",
       " ['play',\n",
       "  'beyond',\n",
       "  'black',\n",
       "  ',',\n",
       "  'mechan',\n",
       "  'chang',\n",
       "  'present',\n",
       "  'expans',\n",
       "  'might',\n"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "voc[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word2Vec model train, save and load\n",
    "\n",
    "The number of feature in my Word2Vec model is 300. The matrix using one-hot expression is about 150k * 2.6M. Curse of dimensionality is gone."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# word2vector\n",
    "num_features = 300    \n",
    "min_word_count = 3     # Frequency < 3 will not be count in.\n",
    "num_workers = 16       \n",
    "context = 8           \n",
    "downsampling = 1e-3   \n",
    "\n",
    "# Initialize and train the model\n",
    "W2Vmodel = Word2Vec(sentences=voc, sg=1, hs=0, workers=num_workers, size=num_features, min_count=min_word_count, window=context,\n",
    "                    sample=downsampling, negative=5, iter=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "151488\n"
     ]
    }
   ],
   "source": [
    "model_voc = set(W2Vmodel.wv.vocab.keys()) \n",
    "print(len(model_voc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model save\n",
    "W2Vmodel.save(\"Word2Vec2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model load\n",
    "W2Vmodel = Word2Vec.load('Word2Vec2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Challenge 2\n",
    "\n",
    "Train the model sentence by sentence is more accurate than the whole review. Cuz the length of the sentence are similar so that the feature of each input is similar. So I did not remove '.' when I remove noise character. That's come from comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence_vectors(model, sentence):\n",
    "    #Collecting all words in the text\n",
    "#     print(sentence)\n",
    "    sent_vector = np.zeros(model.vector_size, dtype=\"float32\")\n",
    "    if sentence == [[]] or sentence == []  :\n",
    "        return sent_vector\n",
    "    words=np.concatenate(sentence)\n",
    "#     words = sentence\n",
    "    #Collecting words that are known to the model\n",
    "    model_voc = set(model.wv.vocab.keys()) \n",
    "#     print(len(model_voc))\n",
    "\n",
    "    # Use a counter variable for number of words in a text\n",
    "    nwords = 0\n",
    "    # Sum up all words vectors that are know to the model\n",
    "    for word in words:\n",
    "        if word in model_voc: \n",
    "            sent_vector += model[word]\n",
    "            nwords += 1.\n",
    "\n",
    "    # Now get the average\n",
    "    if nwords > 0:\n",
    "        sent_vector /= nwords\n",
    "    return sent_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sxy/anaconda3/envs/ML/lib/python3.6/site-packages/ipykernel_launcher.py:19: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n"
     ]
    }
   ],
   "source": [
    "train['sentence_vectors'] = list(map(lambda sen_group:\n",
    "                                      sentence_vectors(W2Vmodel, sen_group),\n",
    "                                      train.tokenized_sentences))\n",
    "test['sentence_vectors'] = list(map(lambda sen_group:\n",
    "                                    sentence_vectors(W2Vmodel, sen_group), \n",
    "                                    test.tokenized_sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectors_to_feats(df, ndim):\n",
    "    index=[]\n",
    "    for i in range(ndim):\n",
    "        df[f'w2v_{i}'] = df['sentence_vectors'].apply(lambda x: x[i])\n",
    "        index.append(f'w2v_{i}')\n",
    "    return df[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = vectors_to_feats(train, 300)\n",
    "X_test = vectors_to_feats(test, 300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.concat([X_train, y_train], axis=1)\n",
    "test = pd.concat([X_test, y_test], axis=1)\n",
    "train.to_csv('train_w2v_100k.csv')\n",
    "test.to_csv('test_w2v_100k.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('train_w2v_1000k.csv').drop(columns = 'Unnamed: 0')\n",
    "test = pd.read_csv('test_w2v_1000k.csv').drop(columns = 'Unnamed: 0')\n",
    "X_train = train.drop(columns = 'rate')\n",
    "X_test = test.drop(columns = 'rate')\n",
    "y_train = train.rate\n",
    "y_test = test.rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>w2v_0</th>\n",
       "      <th>w2v_1</th>\n",
       "      <th>w2v_2</th>\n",
       "      <th>w2v_3</th>\n",
       "      <th>w2v_4</th>\n",
       "      <th>w2v_5</th>\n",
       "      <th>w2v_6</th>\n",
       "      <th>w2v_7</th>\n",
       "      <th>w2v_8</th>\n",
       "      <th>w2v_9</th>\n",
       "      <th>...</th>\n",
       "      <th>w2v_290</th>\n",
       "      <th>w2v_291</th>\n",
       "      <th>w2v_292</th>\n",
       "      <th>w2v_293</th>\n",
       "      <th>w2v_294</th>\n",
       "      <th>w2v_295</th>\n",
       "      <th>w2v_296</th>\n",
       "      <th>w2v_297</th>\n",
       "      <th>w2v_298</th>\n",
       "      <th>w2v_299</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.064735</td>\n",
       "      <td>-0.043941</td>\n",
       "      <td>-0.391560</td>\n",
       "      <td>0.194273</td>\n",
       "      <td>0.038023</td>\n",
       "      <td>-0.062682</td>\n",
       "      <td>-0.003358</td>\n",
       "      <td>-0.220116</td>\n",
       "      <td>0.118839</td>\n",
       "      <td>-0.210516</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.113587</td>\n",
       "      <td>0.034951</td>\n",
       "      <td>-0.048320</td>\n",
       "      <td>-0.084418</td>\n",
       "      <td>-0.016730</td>\n",
       "      <td>0.116862</td>\n",
       "      <td>-0.006845</td>\n",
       "      <td>0.039291</td>\n",
       "      <td>0.216906</td>\n",
       "      <td>-0.068584</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.108961</td>\n",
       "      <td>-0.058336</td>\n",
       "      <td>-0.318453</td>\n",
       "      <td>0.191389</td>\n",
       "      <td>0.005011</td>\n",
       "      <td>-0.072080</td>\n",
       "      <td>0.031846</td>\n",
       "      <td>-0.165923</td>\n",
       "      <td>0.149237</td>\n",
       "      <td>-0.112924</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.165357</td>\n",
       "      <td>0.068865</td>\n",
       "      <td>-0.048133</td>\n",
       "      <td>-0.099376</td>\n",
       "      <td>-0.037351</td>\n",
       "      <td>0.075134</td>\n",
       "      <td>0.002659</td>\n",
       "      <td>0.027652</td>\n",
       "      <td>0.179799</td>\n",
       "      <td>-0.091966</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.037381</td>\n",
       "      <td>-0.105713</td>\n",
       "      <td>-0.223736</td>\n",
       "      <td>0.230134</td>\n",
       "      <td>-0.058320</td>\n",
       "      <td>0.015869</td>\n",
       "      <td>0.157899</td>\n",
       "      <td>-0.395270</td>\n",
       "      <td>0.309151</td>\n",
       "      <td>-0.230134</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.326233</td>\n",
       "      <td>-0.136316</td>\n",
       "      <td>-0.017143</td>\n",
       "      <td>-0.049190</td>\n",
       "      <td>0.112281</td>\n",
       "      <td>0.129845</td>\n",
       "      <td>-0.085892</td>\n",
       "      <td>-0.036840</td>\n",
       "      <td>0.082894</td>\n",
       "      <td>-0.135891</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.073528</td>\n",
       "      <td>-0.035113</td>\n",
       "      <td>-0.295632</td>\n",
       "      <td>0.210620</td>\n",
       "      <td>0.030401</td>\n",
       "      <td>0.020846</td>\n",
       "      <td>0.056935</td>\n",
       "      <td>-0.036678</td>\n",
       "      <td>0.192137</td>\n",
       "      <td>-0.148260</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.098527</td>\n",
       "      <td>-0.010514</td>\n",
       "      <td>-0.077920</td>\n",
       "      <td>-0.030762</td>\n",
       "      <td>0.024684</td>\n",
       "      <td>0.083572</td>\n",
       "      <td>0.047993</td>\n",
       "      <td>0.049659</td>\n",
       "      <td>0.240063</td>\n",
       "      <td>-0.042669</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19995</th>\n",
       "      <td>-0.046645</td>\n",
       "      <td>-0.102675</td>\n",
       "      <td>-0.378800</td>\n",
       "      <td>0.147131</td>\n",
       "      <td>-0.031686</td>\n",
       "      <td>-0.006655</td>\n",
       "      <td>0.074540</td>\n",
       "      <td>-0.169375</td>\n",
       "      <td>0.167047</td>\n",
       "      <td>-0.096239</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.178984</td>\n",
       "      <td>-0.024323</td>\n",
       "      <td>-0.100185</td>\n",
       "      <td>-0.013699</td>\n",
       "      <td>-0.016214</td>\n",
       "      <td>0.134338</td>\n",
       "      <td>0.052931</td>\n",
       "      <td>0.011761</td>\n",
       "      <td>0.187225</td>\n",
       "      <td>-0.053723</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19996</th>\n",
       "      <td>-0.041224</td>\n",
       "      <td>-0.043504</td>\n",
       "      <td>-0.181815</td>\n",
       "      <td>0.241858</td>\n",
       "      <td>-0.087189</td>\n",
       "      <td>0.012512</td>\n",
       "      <td>0.010387</td>\n",
       "      <td>-0.268597</td>\n",
       "      <td>0.120241</td>\n",
       "      <td>-0.173561</td>\n",
       "      <td>...</td>\n",
       "      <td>0.044585</td>\n",
       "      <td>0.042733</td>\n",
       "      <td>0.204142</td>\n",
       "      <td>-0.184483</td>\n",
       "      <td>-0.097213</td>\n",
       "      <td>0.072322</td>\n",
       "      <td>-0.009312</td>\n",
       "      <td>0.044582</td>\n",
       "      <td>0.361448</td>\n",
       "      <td>-0.079877</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19997</th>\n",
       "      <td>0.016242</td>\n",
       "      <td>-0.070195</td>\n",
       "      <td>-0.235203</td>\n",
       "      <td>0.257024</td>\n",
       "      <td>0.072520</td>\n",
       "      <td>-0.119281</td>\n",
       "      <td>-0.028535</td>\n",
       "      <td>-0.243668</td>\n",
       "      <td>0.219881</td>\n",
       "      <td>-0.223677</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.117014</td>\n",
       "      <td>0.083126</td>\n",
       "      <td>0.004575</td>\n",
       "      <td>-0.047602</td>\n",
       "      <td>0.008902</td>\n",
       "      <td>0.131965</td>\n",
       "      <td>-0.026648</td>\n",
       "      <td>-0.042032</td>\n",
       "      <td>0.170854</td>\n",
       "      <td>-0.087977</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19998</th>\n",
       "      <td>-0.088338</td>\n",
       "      <td>0.009536</td>\n",
       "      <td>-0.190505</td>\n",
       "      <td>0.197417</td>\n",
       "      <td>-0.081475</td>\n",
       "      <td>-0.028796</td>\n",
       "      <td>0.044730</td>\n",
       "      <td>-0.118943</td>\n",
       "      <td>0.050266</td>\n",
       "      <td>-0.045812</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.132129</td>\n",
       "      <td>0.220461</td>\n",
       "      <td>0.029903</td>\n",
       "      <td>-0.025690</td>\n",
       "      <td>0.050592</td>\n",
       "      <td>-0.100897</td>\n",
       "      <td>0.093619</td>\n",
       "      <td>0.050197</td>\n",
       "      <td>0.166418</td>\n",
       "      <td>-0.089344</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19999</th>\n",
       "      <td>-0.036372</td>\n",
       "      <td>-0.024210</td>\n",
       "      <td>-0.283933</td>\n",
       "      <td>0.139767</td>\n",
       "      <td>0.035674</td>\n",
       "      <td>-0.090993</td>\n",
       "      <td>0.046099</td>\n",
       "      <td>-0.137280</td>\n",
       "      <td>0.160993</td>\n",
       "      <td>-0.107587</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.109823</td>\n",
       "      <td>0.013688</td>\n",
       "      <td>-0.014184</td>\n",
       "      <td>-0.152064</td>\n",
       "      <td>-0.037780</td>\n",
       "      <td>0.003016</td>\n",
       "      <td>0.037712</td>\n",
       "      <td>-0.028141</td>\n",
       "      <td>0.235616</td>\n",
       "      <td>-0.055234</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>20000 rows × 300 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          w2v_0     w2v_1     w2v_2     w2v_3     w2v_4     w2v_5     w2v_6  \\\n",
       "0     -0.064735 -0.043941 -0.391560  0.194273  0.038023 -0.062682 -0.003358   \n",
       "1     -0.108961 -0.058336 -0.318453  0.191389  0.005011 -0.072080  0.031846   \n",
       "2      0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "3     -0.037381 -0.105713 -0.223736  0.230134 -0.058320  0.015869  0.157899   \n",
       "4     -0.073528 -0.035113 -0.295632  0.210620  0.030401  0.020846  0.056935   \n",
       "...         ...       ...       ...       ...       ...       ...       ...   \n",
       "19995 -0.046645 -0.102675 -0.378800  0.147131 -0.031686 -0.006655  0.074540   \n",
       "19996 -0.041224 -0.043504 -0.181815  0.241858 -0.087189  0.012512  0.010387   \n",
       "19997  0.016242 -0.070195 -0.235203  0.257024  0.072520 -0.119281 -0.028535   \n",
       "19998 -0.088338  0.009536 -0.190505  0.197417 -0.081475 -0.028796  0.044730   \n",
       "19999 -0.036372 -0.024210 -0.283933  0.139767  0.035674 -0.090993  0.046099   \n",
       "\n",
       "          w2v_7     w2v_8     w2v_9  ...   w2v_290   w2v_291   w2v_292  \\\n",
       "0     -0.220116  0.118839 -0.210516  ... -0.113587  0.034951 -0.048320   \n",
       "1     -0.165923  0.149237 -0.112924  ... -0.165357  0.068865 -0.048133   \n",
       "2      0.000000  0.000000  0.000000  ...  0.000000  0.000000  0.000000   \n",
       "3     -0.395270  0.309151 -0.230134  ... -0.326233 -0.136316 -0.017143   \n",
       "4     -0.036678  0.192137 -0.148260  ... -0.098527 -0.010514 -0.077920   \n",
       "...         ...       ...       ...  ...       ...       ...       ...   \n",
       "19995 -0.169375  0.167047 -0.096239  ... -0.178984 -0.024323 -0.100185   \n",
       "19996 -0.268597  0.120241 -0.173561  ...  0.044585  0.042733  0.204142   \n",
       "19997 -0.243668  0.219881 -0.223677  ... -0.117014  0.083126  0.004575   \n",
       "19998 -0.118943  0.050266 -0.045812  ... -0.132129  0.220461  0.029903   \n",
       "19999 -0.137280  0.160993 -0.107587  ... -0.109823  0.013688 -0.014184   \n",
       "\n",
       "        w2v_293   w2v_294   w2v_295   w2v_296   w2v_297   w2v_298   w2v_299  \n",
       "0     -0.084418 -0.016730  0.116862 -0.006845  0.039291  0.216906 -0.068584  \n",
       "1     -0.099376 -0.037351  0.075134  0.002659  0.027652  0.179799 -0.091966  \n",
       "2      0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  \n",
       "3     -0.049190  0.112281  0.129845 -0.085892 -0.036840  0.082894 -0.135891  \n",
       "4     -0.030762  0.024684  0.083572  0.047993  0.049659  0.240063 -0.042669  \n",
       "...         ...       ...       ...       ...       ...       ...       ...  \n",
       "19995 -0.013699 -0.016214  0.134338  0.052931  0.011761  0.187225 -0.053723  \n",
       "19996 -0.184483 -0.097213  0.072322 -0.009312  0.044582  0.361448 -0.079877  \n",
       "19997 -0.047602  0.008902  0.131965 -0.026648 -0.042032  0.170854 -0.087977  \n",
       "19998 -0.025690  0.050592 -0.100897  0.093619  0.050197  0.166418 -0.089344  \n",
       "19999 -0.152064 -0.037780  0.003016  0.037712 -0.028141  0.235616 -0.055234  \n",
       "\n",
       "[20000 rows x 300 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# implement different regression model\n",
    "I implement 4 regression model and compare them with Root Mean Square Error (RMSE) and Mean absolute error(MAE).\n",
    "\n",
    "**RMSE:** Root Mean Square Error (RMSE) is the standard deviation of the residuals (prediction errors). Residuals are a measure of how far from the regression line data points are; RMSE is a measure of how spread out these residuals are. It can tells you how concentrated the data is around the line of best fit. \n",
    "\n",
    "**MAE:**  Mean absolute error (MAE) is a measure of errors between paired observations expressing the same phenomenon. It is thus an arithmetic average of the absolute errors |ei|=|yi-xi|, where yi is the prediction and xi the true value. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear regression model\n",
    "Linear regression is a basic and commonly used type of predictive analysis. Parameter calculation of linear equation using least squares method.\n",
    "\n",
    "[Linear regression introduction](https://machinelearningmastery.com/linear-regression-for-machine-learning/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearRegression(copy_X=True, fit_intercept=True, n_jobs=None, normalize=False)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_lr = LinearRegression()\n",
    "model_lr.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_y_predict=model_lr.predict(X_test)\n",
    "y_test = np.array(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "linear_regression_rmse =  1.5985795417920345\n",
      "linear_regression_mae =  1.2179818164120766\n"
     ]
    }
   ],
   "source": [
    "# (RMSE)\n",
    "rmse = np.sqrt(mean_squared_error(y_test,lr_y_predict))\n",
    "\n",
    "# (MAE)\n",
    "mae = mean_absolute_error(y_test, lr_y_predict)\n",
    "\n",
    "print('linear_regression_rmse = ', rmse)\n",
    "print('linear_regression_mae = ', mae)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['save/model_lr.pkl']"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib.dump(model_lr, 'save/model_lr.pkl')\n",
    "\n",
    "# model_lr = joblib.load('save/model_lr_1000k.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVR model\n",
    "Support vector regression(SVR) is an application of support vector machine(SVM) to regression problem.\n",
    "\n",
    "Regression is like looking for the internal relationship of a bunch of data. Regardless of whether the pile of data consists of several categories, a formula is obtained to fit these data. When a new coordinate value is given, a new value can be obtained. So for SVR, it is to find a face or a function, and you can fit all the data (that is, all data points, regardless of the type, the closest distance from the data point to the face or function)\n",
    "\n",
    "[SVR introduction introduction](https://towardsdatascience.com/an-introduction-to-support-vector-regression-svr-a3ebc1672c2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVR(C=1.0, cache_size=200, coef0=0.0, degree=3, epsilon=0.1, gamma='scale',\n",
       "    kernel='rbf', max_iter=-1, shrinking=True, tol=0.001, verbose=False)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_svm = SVR()\n",
    "model_svm.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_y_predict=model_svm.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "svm_rmse =  1.4967321667740556\n",
      "svm_mae =  1.1245787830283758\n"
     ]
    }
   ],
   "source": [
    "# (RMSE)\n",
    "rmse = np.sqrt(mean_squared_error(y_test,svm_y_predict))\n",
    "\n",
    "# (MAE)\n",
    "mae = mean_absolute_error(y_test, svm_y_predict)\n",
    "\n",
    "print('svm_rmse = ', rmse)\n",
    "print('svm_mae = ', mae)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['save/model_svm.pkl']"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib.dump(model_lr, 'save/model_svm.pkl')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bayesian Ridge model\n",
    "In the Bayesian viewpoint, we formulate linear regression using probability distributions rather than point estimates. The response, y, is not estimated as a single value, but is assumed to be drawn from a probability distribution.\n",
    "\n",
    "The output, y is generated from a normal (Gaussian) Distribution characterized by a mean and variance. The mean for linear regression is the transpose of the weight matrix multiplied by the predictor matrix. The variance is the square of the standard deviation σ (multiplied by the Identity matrix because this is a multi-dimensional formulation of the model).\n",
    "\n",
    "[Bayesian Ridge introduction](https://towardsdatascience.com/introduction-to-bayesian-linear-regression-e66e60791ea7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BayesianRidge(alpha_1=1e-06, alpha_2=1e-06, alpha_init=None,\n",
       "              compute_score=False, copy_X=True, fit_intercept=True,\n",
       "              lambda_1=1e-06, lambda_2=1e-06, lambda_init=None, n_iter=300,\n",
       "              normalize=False, tol=0.001, verbose=False)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_bayes_ridge = BayesianRidge()\n",
    "model_bayes_ridge.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "bayes_y_predict = model_bayes_ridge.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BayesianRidge_rmse =  1.5980023290295695\n",
      "BayesianRidge_mae =  1.2175385536747287\n"
     ]
    }
   ],
   "source": [
    "# (RMSE)\n",
    "rmse = np.sqrt(mean_squared_error(y_test,bayes_y_predict))\n",
    "\n",
    "# (MAE)\n",
    "mae = mean_absolute_error(y_test, bayes_y_predict)\n",
    "\n",
    "print('BayesianRidge_rmse = ', rmse)\n",
    "print('BayesianRidge_mae = ', mae)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['save/model_bayes.pkl']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib.dump(model_bayes_ridge, 'save/model_bayes.pkl')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest Regression model\n",
    "\n",
    "Random forest is a bagging technique and not a boosting technique. The trees in random forests are run in parallel. There is no interaction between these trees while building the trees.\n",
    "\n",
    "The throught of Random Forest Regression is using the Boosting and ensemble in decision tree. In the lecture mentioned.\n",
    "\n",
    "[Random Forest Regression introduction](https://towardsdatascience.com/random-forest-and-its-implementation-71824ced454f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestRegressor(bootstrap=True, ccp_alpha=0.0, criterion='mse',\n",
       "                      max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "                      max_samples=None, min_impurity_decrease=0.0,\n",
       "                      min_impurity_split=None, min_samples_leaf=1,\n",
       "                      min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "                      n_estimators=20, n_jobs=None, oob_score=False,\n",
       "                      random_state=None, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_random_forest_regressor = ensemble.RandomForestRegressor(n_estimators=20)\n",
    "model_random_forest_regressor.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_forest_y_predict = model_random_forest_regressor.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BayesianRidge_rmse =  1.6054778376150676\n",
      "BayesianRidge_mae =  1.2233214206573564\n"
     ]
    }
   ],
   "source": [
    "# (RMSE)\n",
    "rmse = np.sqrt(mean_squared_error(y_test,random_forest_y_predict))\n",
    "\n",
    "# (MAE)\n",
    "mae = mean_absolute_error(y_test, random_forest_y_predict)\n",
    "\n",
    "print('BayesianRidge_rmse = ', rmse)\n",
    "print('BayesianRidge_mae = ', mae)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['save/model_random_forest.pkl']"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib.dump(model_random_forest_regressor, 'save/model_random_forest.pkl')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict function for one review with four model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(text):\n",
    "    model_lr = joblib.load('save/model_lr.pkl')\n",
    "    model_svm = joblib.load('save/model_svm.pkl')\n",
    "    model_random_forest_regressor = joblib.load('save/model_random_forest.pkl')\n",
    "    model_bayes_ridge = joblib.load('save/model_bayes.pkl')\n",
    "    data = {'comment': Series(text)}\n",
    "    data = pd.DataFrame(data)\n",
    "    print(data)\n",
    "    data['comment'] = data['comment'].apply(remove_between_square_brackets)\n",
    "    data['comment'] = data['comment'].apply(remove_special_characters)\n",
    "    data['comment'] = data['comment'].apply(simple_stemmer)\n",
    "    data['comment'] = data['comment'].apply(remove_stopwords)\n",
    "\n",
    "    data['comment'] = data.comment.str.lower()\n",
    "    data['document_sentences'] = data.comment.str.split('.')\n",
    "    data['tokenized_sentences'] = data['document_sentences']\n",
    "    data['tokenized_sentences'] = list(\n",
    "        map(lambda sentences: list(map(nltk.word_tokenize, sentences)), data.document_sentences))\n",
    "    data['tokenized_sentences'] = list(\n",
    "        map(lambda sentences: list(filter(lambda lst: lst, sentences)), data.tokenized_sentences))\n",
    "    print(data)\n",
    "    # sentence = data['tokenized_sentences'][0]\n",
    "    W2Vmodel = Word2Vec.load(\"Word2Vec2\")\n",
    "\n",
    "    data['sentence_vectors'] = list(map(lambda sen_group:\n",
    "                                        sentence_vectors(W2Vmodel, sen_group),\n",
    "                                        data.tokenized_sentences))\n",
    "    text = vectors_to_feats(data, 300)\n",
    "    print(text)\n",
    "    lr_y_predict = model_lr.predict(text)\n",
    "    svm_y_predict = model_svm.predict(text)\n",
    "    bayes_y_predict = model_bayes_ridge.predict(text)\n",
    "    random_forest_y_predict = model_random_forest_regressor.predict(text)\n",
    "\n",
    "    return lr_y_predict, svm_y_predict, random_forest_y_predict, bayes_y_predict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                             comment\n",
      "0  This is a great game.  I've even got a number ...\n",
      "1  This is a great game.  I've even got a number ...\n",
      "                                             comment  \\\n",
      "0  thi great game ive even got number non game pl...   \n",
      "1  thi great game ive even got number non game pl...   \n",
      "\n",
      "                                  document_sentences  \\\n",
      "0  [thi great game ive even got number non game p...   \n",
      "1  [thi great game ive even got number non game p...   \n",
      "\n",
      "                                 tokenized_sentences  \n",
      "0  [[thi, great, game, ive, even, got, number, no...  \n",
      "1  [[thi, great, game, ive, even, got, number, no...  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sxy/anaconda3/envs/ML/lib/python3.6/site-packages/ipykernel_launcher.py:19: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      w2v_0     w2v_1     w2v_2     w2v_3     w2v_4     w2v_5     w2v_6  \\\n",
      "0 -0.052897 -0.077122 -0.441616  0.210372  0.019172 -0.060663  0.048674   \n",
      "1 -0.052897 -0.077122 -0.441616  0.210372  0.019172 -0.060663  0.048674   \n",
      "\n",
      "      w2v_7     w2v_8     w2v_9  ...   w2v_290  w2v_291  w2v_292   w2v_293  \\\n",
      "0 -0.169603  0.132948 -0.137659  ... -0.135482   0.0026 -0.05121 -0.148072   \n",
      "1 -0.169603  0.132948 -0.137659  ... -0.135482   0.0026 -0.05121 -0.148072   \n",
      "\n",
      "    w2v_294  w2v_295   w2v_296   w2v_297   w2v_298  w2v_299  \n",
      "0 -0.029361  0.08649 -0.070255 -0.040144  0.108867 -0.01677  \n",
      "1 -0.029361  0.08649 -0.070255 -0.040144  0.108867 -0.01677  \n",
      "\n",
      "[2 rows x 300 columns]\n",
      "(array([8.09318704, 8.09318704]), array([8.09318704, 8.09318704]), array([8.41475, 8.41475]), array([8.06230953, 8.06230953]))\n"
     ]
    }
   ],
   "source": [
    "print(predict([\"This is a great game.  I've even got a number of non game players enjoying it.  Fast to learn and always changing.\",\n",
    "        \"This is a great game.  I've even got a number of non game players enjoying it.  Fast to learn and always changing.\"]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
