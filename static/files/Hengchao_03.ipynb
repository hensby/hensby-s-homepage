{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 3 Naive bayes classifier\n",
    "### Hengchao Wang 1001778272\n",
    " \n",
    "platform: \n",
    "I7-9700k GTX-1080ti"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import string\n",
    "import csv\n",
    "# from bs4 import BeautifulSoup \n",
    "import math\n",
    "import random\n",
    "from queue import PriorityQueue as PQueue  # priorityQueue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get data from txt\n",
    "Some of my thoughts of data pre-processing come from my own homework in my Machine learning class. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# constant. dictionary of files\n",
    "train_positive_file_dir = 'aclImdb/train/pos'\n",
    "train_negitive_file_dir = 'aclImdb/train/neg'\n",
    "\n",
    "test_positive_file_dir = 'aclImdb/test/pos'\n",
    "test_negitive_file_dir = 'aclImdb/test/neg'\n",
    "\n",
    "train_unsup_file_dir = 'aclImdb/train/unsup'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def file_name(file_dir):   \n",
    "    files_name = os.listdir(file_dir)\n",
    "    return files_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save filename in txt\n",
    "def save_name(file_dir, name):\n",
    "    f = open(name + '.txt' ,'w')  # 'a' add not reset.\n",
    "    \n",
    "    files_name = file_name(file_dir)\n",
    "    for i in files_name:        \n",
    "        f.write(i)  # string\n",
    "        f.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_name(train_positive_file_dir, 'train_positive_file_dir')\n",
    "save_name(train_negitive_file_dir, 'train_negitive_file_dir')\n",
    "\n",
    "save_name(test_positive_file_dir, 'test_positive_file_dir')\n",
    "save_name(test_negitive_file_dir, 'test_negitive_file_dir')\n",
    "\n",
    "save_name(train_unsup_file_dir, 'train_unsup_file_dir')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get filename from txt\n",
    "def get_data(filename):\n",
    "    f = open(filename +'.txt')\n",
    "    file_names = []\n",
    "    for i in f.readlines():\n",
    "        file_names.append(i.replace(\"\\n\", \"\"))\n",
    "    return file_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# My thoughts of this part come from my own homework in my Machine learning class. \n",
    "# remove some useless charactor at beginning\n",
    "# load contents into dict\n",
    "def load_data(fileList, url):\n",
    "    sentenseList = []\n",
    "    pa = string.punctuation\n",
    "    for file in fileList:        \n",
    "        with open(url +\"/\"+ file,errors='ignore') as f:\n",
    "            ori_data = f.read().lower()\n",
    "            data1 = re.sub('\\n{2,6}','  ',ori_data)\n",
    "            data2 = re.sub('\\n',' ',data1)\n",
    "            data3 = re.sub('  ','yxw ',data2)\n",
    "            data4 = re.sub(\"[%s]+\"%('\"|#|$|%|&|\\|(|)|*|+|-|/|<|=|>|@|^|`|{|}|~'), \"\", data3)\n",
    "            sentense = re.sub(\"[%s]+\"%('.|?|!|:|;'),'   ',data4)\n",
    "            sentenseList.append(sentense)\n",
    "    return sentenseList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_names_train_pos = get_data('train_positive_file_dir')\n",
    "file_names_train_neg = get_data('train_negitive_file_dir')\n",
    "\n",
    "file_names_test_pos = get_data('test_positive_file_dir')\n",
    "file_names_test_neg = get_data('test_negitive_file_dir')\n",
    "\n",
    "file_names_train_unsup = get_data('train_unsup_file_dir')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sentenseList1 = load_data(file_names_train_pos, train_positive_file_dir) \n",
    "train_sentenseList2 = load_data(file_names_train_neg, train_negitive_file_dir) \n",
    "\n",
    "test_sentenseList1 = load_data(file_names_test_pos, test_positive_file_dir) \n",
    "test_sentenseList2 = load_data(file_names_test_neg, test_negitive_file_dir) \n",
    "\n",
    "train_unsup_sentenseList = load_data(file_names_train_unsup, train_unsup_file_dir) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge data\n",
    "train_target1 = [1]*len(train_sentenseList1)\n",
    "train_target2 = [0]*len(train_sentenseList2)\n",
    "train_target = train_target1 + train_target2\n",
    "\n",
    "train_text1 = train_sentenseList1\n",
    "train_text2 = train_sentenseList2\n",
    "train_text = train_text1 + train_text2\n",
    "\n",
    "test_target1 = [1]*len(test_sentenseList1)\n",
    "test_target2 = [0]*len(test_sentenseList2)\n",
    "test_target = test_target1 + test_target2\n",
    "\n",
    "test_text1 = test_sentenseList1\n",
    "test_text2 = test_sentenseList2\n",
    "test_text = test_text1 + test_text2\n",
    "\n",
    "train_unsup_target = [0]*len(train_unsup_sentenseList)\n",
    "train_unsup_text = train_unsup_sentenseList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_to_dict = {'content':train_text, 'target':train_target}\n",
    "test_to_dict = {'content':test_text, 'target':test_target}\n",
    "\n",
    "train_unsup_to_dict = {'content':train_unsup_text, 'target':train_unsup_target}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove stopwords and useless symbol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy the stopwords from wordcloud.\n",
    "# My thoughts of this part come from my own homework in my Machine learning class. \n",
    "stopSet = set({'did', 'such', 'doing', 'down', 'me', 'just', 'very', 'shan', 'against', 't', \"you're\", \n",
    "          'only', \"haven't\", 'yours', 'you', 'its', 'other', 'we', 'where', 'then', 'they', 'won', \"you've\",\n",
    "          'some', 've', 'y', 'each', \"you'll\", 'them', 'to', 'was', 'once', 'and', 'ain', 'under', 'through',\n",
    "          'for', \"won't\", 'mustn', 'a', 'are', 'that', 'at', 'why', 'any', 'nor', 'these', 'yourselves',\n",
    "          'has', 'here', \"needn't\", 'm', 'above', 'up', 'more', 'if', 'ma', 'didn', 'whom', 'can', 'have',\n",
    "          'an', 'should', 'there', 'couldn', 'her', 'how', 'of', 'doesn', \"shouldn't\", 'further', \n",
    "          \"wasn't\", 'between', 'd', 'wouldn', 'his', 'being', 'do', 'when', 'hasn', \"she's\", 'by', \"should've\",\n",
    "          'into', 'aren', 'weren', 'as', 'needn', 'what', \"it's\", 'hadn', 'with', 'after', 'he', 'off', 'not',\n",
    "          'does', 'own', \"weren't\", \"isn't\", 'my', 'too', \"wouldn't\", 'been', 'again', 'same', 'few', \"don't\",\n",
    "          'our', 'myself', 'your', 'before', 'about', 'most', 'during', 'll', 'on', 'shouldn', 'is', 'out',\n",
    "          \"shan't\", 'below', 'which', 'from', 'she', 'were', 'those', 'over', 'until', 'theirs', 'mightn',\n",
    "          'yourself', 'i', 'am', 'so', 'himself', 'it', 'had', 'or', 'all', 'while', \"aren't\", 'ours',\n",
    "          \"that'll\", 'but', 'because', 'in', 'now', 'themselves', 'him', \"doesn't\", 'both', 're', 'wasn',\n",
    "          's', \"hasn't\", \"didn't\", 'their', \"mustn't\", 'herself', 'the', 'this', 'will', 'isn', \"you'd\", \n",
    "          'haven', 'itself', \"couldn't\", 'o', 'be', 'don', 'hers', \"mightn't\", 'having', \"hadn't\", 'ourselves',\n",
    "          'who', 'than', 'br'})\n",
    "\n",
    "# # Remove html characters. I used BeautifulSoup at this part but it's ok to remove this function.\n",
    "# # So I just put it here as a comment but not use it.\n",
    "\n",
    "# def strip_html(text):\n",
    "#     soup = BeautifulSoup(text, \"html.parser\")\n",
    "#     return soup.get_text()\n",
    "\n",
    "#Removing the square brackets\n",
    "def remove_between_square_brackets(text):\n",
    "    return re.sub('\\[[^]]*\\]', '', text)\n",
    "\n",
    "def remove_special_characters(text, remove_digits=True):\n",
    "    pattern=r'[^a-zA-z0-9\\s]'\n",
    "    text=re.sub(pattern,'',text)\n",
    "    return text\n",
    "\n",
    "def remove_stopwords(text, is_lower_case=False):\n",
    "#     print(text)\n",
    "    tokens = text.split();\n",
    "    if is_lower_case:\n",
    "        filtered_tokens = [token for token in tokens if token not in stopSet]\n",
    "    else:\n",
    "        filtered_tokens = [token for token in tokens if token.lower() not in stopSet]\n",
    "    filtered_text = ' '.join(filtered_tokens)    \n",
    "    return filtered_text\n",
    "\n",
    "#Removing the noisy text\n",
    "def denoise_text(text):\n",
    "#     text = strip_html(text)\n",
    "    text = remove_between_square_brackets(text)\n",
    "    text = remove_special_characters(text)\n",
    "    text = remove_stopwords(text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "based on an actual story, john boorman shows the struggle of an american doctor, whose husband and son were murdered and she was continually plagued with her loss    a holiday to burma with her sister seemed like a good idea to get away from it all, but when her passport was stolen in rangoon, she could not leave the country with her sister, and was forced to stay back until she could get i   d    papers from the american embassy    to fill in a day before she could fly out, she took a trip into the countryside with a tour guide    i tried finding something in those stone statues, but nothing stirred in me    i was stone myself    br br suddenly all hell broke loose and she was caught in a political revolt    just when it looked like she had escaped and safely boarded a train, she saw her tour guide get beaten and shot    in a split second she decided to jump from the moving train and try to rescue him, with no thought of herself    continually her life was in danger    br br here is a woman who demonstrated spontaneous, selfless charity, risking her life to save another    patricia arquette is beautiful, and not just to look at    she has a beautiful heart    this is an unforgettable story    br br we are taught that suffering is the one promise that life always keeps   \n"
     ]
    }
   ],
   "source": [
    "# make a copy of data dict\n",
    "train_to_dict_tmp = {'content':[], 'target':[]}\n",
    "test_to_dict_tmp = {'content':[], 'target':[]}\n",
    "unsup_to_dict_tmp = {'content':[], 'target':[]}\n",
    "\n",
    "for i in range(len(train_to_dict['content'])):\n",
    "    train_to_dict_tmp['content'].append(denoise_text(train_to_dict['content'][i]))\n",
    "    train_to_dict_tmp['target'] = train_to_dict['target']\n",
    "\n",
    "for i in range(len(test_to_dict['content'])):\n",
    "    test_to_dict_tmp['content'].append(denoise_text(test_to_dict['content'][i]))\n",
    "    test_to_dict_tmp['target'] = test_to_dict['target']\n",
    "\n",
    "for i in range(len(train_unsup_to_dict['content'])):\n",
    "    unsup_to_dict_tmp['content'].append(denoise_text(train_unsup_to_dict['content'][i]))\n",
    "    unsup_to_dict_tmp['target'] = train_unsup_to_dict['target']\n",
    "\n",
    "print(test_to_dict_tmp['target'][0])\n",
    "print(test_to_dict['content'][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define storage of word dictionary and occurrence time\n",
    "wordSet = []     # store word as a set\n",
    "word_dictionary = {}  #dictionary, {Integer->index: String->word: }\n",
    "word_count = {}    #count the frequency of each word, {Integer->index: Integer->frequency}\n",
    "word_occurrence = {}   # count num of documents containing current word.\n",
    "word_occurrence_pos = {}    # count num of positive documents containing current word.\n",
    "word_occurrence_neg = {}    # count num of negative documents containing current word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_word_set(wordSet, word_dict):\n",
    "    for text in word_dict['content']:\n",
    "        text_words = text.split()\n",
    "        for word in text_words:\n",
    "            wordSet.append(word);\n",
    "    return wordSet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "wordSet = get_word_set(wordSet, train_to_dict_tmp)\n",
    "wordSet = get_word_set(wordSet, unsup_to_dict_tmp)\n",
    "wordSet = set(wordSet)\n",
    "# wordSet is a set contains all words. Exclude duplicates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build dictionary\n",
    "count = 0    # index\n",
    "for i in wordSet:\n",
    "    word_dictionary[i] = count\n",
    "    count = count + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "199911"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(word_dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get occurrence time for each word.\n",
    "def get_count(word_dictionary, word_count, wordSet, word_dict):\n",
    "    for text in word_dict['content']:\n",
    "        text_words = text.split()\n",
    "        for word in text_words:\n",
    "#             print(word)\n",
    "            if word in word_dictionary.keys():\n",
    "                if word_dictionary[word] not in word_count.keys():\n",
    "                    word_count[word_dictionary[word]] = 1\n",
    "                else: \n",
    "                    word_count[word_dictionary[word]] = word_count[word_dictionary[word]] + 1\n",
    "    return word_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_count = get_count(word_dictionary, word_count, wordSet, train_to_dict_tmp)\n",
    "word_count = get_count(word_dictionary, word_count, wordSet, unsup_to_dict_tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "47696"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# omit rare words if the occurrence is less than five times\n",
    "for word in wordSet:\n",
    "    if word_count[word_dictionary[word]] <= 5:\n",
    "        word_count.pop(word_dictionary[word])\n",
    "        word_dictionary\n",
    "        \n",
    "len(word_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate probability and conditional probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cauculate occurrence of each word in pos ang neg. calculate word_occurrence_pos{} and word_occurrence_neg{}\n",
    "def get_occurrence(word_dict, word_dictionary, wordSet, word_occurrence, word_occurrence_pos, word_occurrence_neg):\n",
    "    doc_number = 0 \n",
    "    \n",
    "    for i in range(len(word_dict['content'])):\n",
    "        text = word_dict['content'][i]\n",
    "        target = word_dict['target'][i]\n",
    "#         print(target)\n",
    "        doc_number += 1\n",
    "        text_word_set = set(text.split())\n",
    "        for word in text_word_set:\n",
    "            tmp = word_dictionary[word]\n",
    "            if tmp in word_occurrence.keys(): \n",
    "                word_occurrence[tmp] += 1\n",
    "            else: \n",
    "                word_occurrence[tmp] = 1\n",
    "            if target == 0:\n",
    "                if tmp in word_occurrence_neg.keys(): \n",
    "                   word_occurrence_neg[tmp] += 1\n",
    "                else: \n",
    "                   word_occurrence_neg[tmp] = 1\n",
    "            if target == 1:\n",
    "                if tmp in word_occurrence_pos.keys(): \n",
    "                   word_occurrence_pos[tmp] += 1\n",
    "                else: \n",
    "                   word_occurrence_pos[tmp] = 1\n",
    "    return word_occurrence, word_occurrence_pos, word_occurrence_neg, doc_number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get conditional probability with laplace smoothing \n",
    "def get_condition_laplace(word, word_dictionary, word_occurrence, word_occurrence_pos, word_occurrence_neg, size_of_data):\n",
    "    tmp = word_dictionary[word]\n",
    "    conditional_probability_pos = 0\n",
    "    conditional_probability_neg = 0\n",
    "    if tmp in word_occurrence_pos.keys():\n",
    "        # Laplace Smoothing\n",
    "        conditional_probability_pos = float(word_occurrence_pos[tmp] + 1)/ float(size_of_data + 2)\n",
    "    else: conditional_probability_pos = float(1)/ float(size_of_data + 2)\n",
    "    if tmp in word_occurrence_neg.keys():\n",
    "        conditional_probability_neg = float(word_occurrence_neg[tmp] + 1)/ float(size_of_data + 2)\n",
    "    else: conditional_probability_neg = float(1)/ float(size_of_data + 2)\n",
    "    return conditional_probability_pos, conditional_probability_neg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get conditional probability with m estimate smoothing\n",
    "def get_condition_m_estimate(word, word_dictionary, word_occurrence, word_occurrence_pos, word_occurrence_neg, size_of_data, m):\n",
    "    tmp = word_dictionary[word]\n",
    "    conditional_probability_pos = 0\n",
    "    conditional_probability_neg = 0\n",
    "    if tmp in word_occurrence_pos.keys():\n",
    "        conditional_probability_pos = float(word_occurrence_pos[tmp] + m*(0.5))/ float(size_of_data + m)\n",
    "    else: conditional_probability_pos = float(m*(0.5))/ float(size_of_data + m)\n",
    "    if tmp in word_occurrence_neg.keys():\n",
    "        conditional_probability_neg = float(word_occurrence_neg[tmp] + m*(0.5))/ float(size_of_data + m)\n",
    "    else: conditional_probability_neg = float(m*(0.5))/ float(size_of_data + m)\n",
    "    return conditional_probability_pos, conditional_probability_neg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compare conditional_probability\n",
    "def naive_bayes(text, word_dictionary, word_occurrence, word_occurrence_pos, word_occurrence_neg, size_of_data):\n",
    "    pro_pos = 0\n",
    "    pro_neg = 0\n",
    "    for word in text.split():\n",
    "        if word in word_dictionary.keys():\n",
    "            tmp = word_dictionary[word]\n",
    "            if tmp in word_count.keys() and tmp in word_occurrence.keys():\n",
    "                # laplace\n",
    "#                 conditional_probability_pos, conditional_probability_neg \\\n",
    "#                 = get_condition_laplace(word, word_dictionary, word_occurrence, word_occurrence_pos, word_occurrence_neg, size_of_data)\n",
    "                # m estimate, m = 1\n",
    "#                 conditional_probability_pos, conditional_probability_neg \\\n",
    "#                 = get_condition_m_estimate(word, word_dictionary, word_occurrence, word_occurrence_pos, word_occurrence_neg, size_of_data, 1)\n",
    "                # m estimate, m = 0.5            \n",
    "#                 conditional_probability_pos, conditional_probability_neg \\\n",
    "#                 = get_condition_m_estimate(word, word_dictionary, word_occurrence, word_occurrence_pos, word_occurrence_neg, size_of_data, 0.5)\n",
    "                # m estimate, m = 0.2     \n",
    "#                 conditional_probability_pos, conditional_probability_neg \\\n",
    "#                 = get_condition_m_estimate(word, word_dictionary, word_occurrence, word_occurrence_pos, word_occurrence_neg, size_of_data, 0.2)\n",
    "                # m estimate, m = 0.1 \n",
    "#                 conditional_probability_pos, conditional_probability_neg \\\n",
    "#                 = get_condition_m_estimate(word, word_dictionary, word_occurrence, word_occurrence_pos, word_occurrence_neg, size_of_data, 0.1)\n",
    "                # m estimate, m = 0.01\n",
    "                conditional_probability_pos, conditional_probability_neg \\\n",
    "                = get_condition_m_estimate(word, word_dictionary, word_occurrence, word_occurrence_pos, word_occurrence_neg, size_of_data, 0.01)\n",
    "                pro_pos += math.log(conditional_probability_pos)\n",
    "                pro_neg += math.log(conditional_probability_neg)\n",
    "#     print(pro_neg, pro_pos)\n",
    "    if pro_neg > pro_pos: return 0\n",
    "    return 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_accuracy(word_dict, word_dictionary, word_occurrence, word_occurrence_pos, word_occurrence_neg):\n",
    "    count = 0\n",
    "    score = 0\n",
    "    for i in range(len(word_dict['content'])):\n",
    "        count += 1\n",
    "        text = word_dict['content'][i]\n",
    "        target = word_dict['target'][i]\n",
    "        res = naive_bayes(text, word_dictionary, word_occurrence, word_occurrence_pos, word_occurrence_neg, len(word_dict['content']))\n",
    "        if target == res: \n",
    "            score += 1\n",
    "    return float(score)/ float(count) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate accuracy using dev dataset \n",
    "### compare smoothing methods: Laplace and m estimate smoothing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fold_validation(word_dict, n_fold):\n",
    "    dataset_split = []\n",
    "    text = word_dict['content']\n",
    "    fold_size = len(text)// n_fold\n",
    "    index_list = random.sample(range(0,len(text)),len(text))\n",
    "#     print(index)\n",
    "    for i in range (n_fold):\n",
    "        tmp_dict = {}\n",
    "        tmp_content = []\n",
    "        tmp_target = []\n",
    "        for index in index_list[i * fold_size: (i + 1)* fold_size]:\n",
    "            tmp_content.append(text[index])\n",
    "            tmp_target.append(word_dict['target'][index])\n",
    "        tmp_dict = {'content':tmp_content, 'target': tmp_target}\n",
    "        dataset_split.append(tmp_dict)\n",
    "    return dataset_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5 folds validation\n",
    "n_fold = 5\n",
    "data_after_n_fold = fold_validation(train_to_dict_tmp, n_fold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.9272, 0.9366, 0.9266, 0.9282, 0.9414]\n",
      "0.932\n"
     ]
    }
   ],
   "source": [
    "# using development dataset \n",
    "# compare m-estimate and laplace smoothing. edit naive_bayes() line 8-13 \n",
    "score = []\n",
    "for i in range(n_fold):\n",
    "    train_dict_final = {}\n",
    "    for tmp in range(n_fold):\n",
    "        tmp_content = []\n",
    "        tmp_target = []\n",
    "        if tmp != i: \n",
    "            tmp_content = tmp_content + data_after_n_fold[tmp]['content']\n",
    "            tmp_target = tmp_target + data_after_n_fold[tmp]['target']\n",
    "    train_dict_final = {'content':tmp_content, 'target': tmp_target}\n",
    "    test_dict_final = data_after_n_fold[i]\n",
    "    word_occurrence, word_occurrence_pos, word_occurrence_neg, doc_number \\\n",
    "    = get_occurrence(train_dict_final, word_dictionary, wordSet, word_occurrence,\\\n",
    "                  word_occurrence_pos, word_occurrence_neg)\n",
    "    score.append(get_accuracy(test_dict_final, word_dictionary, word_occurrence, word_occurrence_pos, word_occurrence_neg))\n",
    "print(score)\n",
    "mean_score = 0\n",
    "for i in score:\n",
    "     mean_score += i\n",
    "print(float(mean_score)/ len(score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### result of laplace smoothing\n",
    "[0.9076, 0.9132, 0.9054, 0.9012, 0.9248]\n",
    "0.91044\n",
    "\n",
    "### result of m estimate smoothing, m = 1\n",
    "[0.911, 0.9092, 0.9138, 0.9108, 0.9316]\n",
    "0.9152799999999999\n",
    "\n",
    "### result of m estimate smoothing, m = 0.5\n",
    "[0.9072, 0.9152, 0.9172, 0.9184, 0.9364]\n",
    "0.91888\n",
    "\n",
    "### result of m estimate smoothing, m = 0.2\n",
    "[0.9222, 0.921, 0.921, 0.9176, 0.9244]\n",
    "0.9212400000000001\n",
    "\n",
    "### result of m estimate smoothing, m = 0.1\n",
    "[0.9212, 0.9222, 0.9196, 0.9168, 0.9384]\n",
    "0.92364\n",
    "\n",
    "### result of m estimate smoothing, m = 0.01\n",
    "[0.9272, 0.9366, 0.9266, 0.9282, 0.9414]\n",
    "0.932\n",
    "\n",
    "# conclusion: m estimate smoothing have better result. And less m, better result. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Derive Top 10 words that predicts positive and negative class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_10(word_dict, word_dictionary, word_occurrence, word_occurrence_pos, word_occurrence_neg):\n",
    "    top_10_pos = PQueue()  # priorityQueue\n",
    "    top_10_neg = PQueue()\n",
    "    for word in word_dictionary.keys():\n",
    "        if word in word_dictionary.keys():\n",
    "            tmp = word_dictionary[word]\n",
    "            if tmp in word_count.keys() and tmp in word_occurrence.keys():\n",
    "                conditional_probability_pos, conditional_probability_neg \\\n",
    "                = get_condition_laplace(word, word_dictionary, word_occurrence, word_occurrence_pos, word_occurrence_neg, len(word_dict['content']))\n",
    "                top_10_pos.put([conditional_probability_pos * -1, word])\n",
    "#                 if top_10_pos.qsize() > 10: top_10_pos.get()[-1]\n",
    "                top_10_neg.put([conditional_probability_neg * -1, word])\n",
    "#                 if top_10_neg.qsize() > 10: top_10_neg.get()[-1]\n",
    "    return top_10_pos, top_10_neg\n",
    "top_10_pos, top_10_neg = get_top_10(train_to_dict ,word_dictionary, word_occurrence, word_occurrence_pos, word_occurrence_neg)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 words that predicts positive\n",
      "one\n",
      "movie\n",
      "film\n",
      "like\n",
      "good\n",
      "time\n",
      "great\n",
      "see\n",
      "story\n",
      "well\n",
      "\n",
      "\n",
      "Top 10 words that predicts negative\n",
      "movie\n",
      "one\n",
      "film\n",
      "like\n",
      "no\n",
      "even\n",
      "good\n",
      "would\n",
      "bad\n",
      "time\n"
     ]
    }
   ],
   "source": [
    "print('Top 10 words that predicts positive')\n",
    "i = 10\n",
    "while i > 0:\n",
    "    print(top_10_pos.get(-1)[1])\n",
    "    i -= 1\n",
    "print('\\n')\n",
    "print('Top 10 words that predicts negative')\n",
    "i = 10\n",
    "while i > 0:\n",
    "    print(top_10_neg.get(-1)[1])\n",
    "    i -= 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using the test dataset calculate the final accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.92964"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# final accuracy using m estimate smoothing\n",
    "word_occurrence, word_occurrence_pos, word_occurrence_neg, doc_number \\\n",
    "    = get_occurrence(train_to_dict_tmp, word_dictionary, wordSet, word_occurrence,\\\n",
    "                  word_occurrence_pos, word_occurrence_neg)\n",
    "   \n",
    "score = get_accuracy(train_to_dict_tmp, word_dictionary, word_occurrence, word_occurrence_pos, word_occurrence_neg)\n",
    "score\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "final accuracy using m estimate smoothing = 0.92964"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
